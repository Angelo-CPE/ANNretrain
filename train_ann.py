# -*- coding: utf-8 -*-
"""Train ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13aSRxdCH4gieBIyh9UuliQLCjX9q2fXV
"""

import os
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from skimage.feature import hog
from scipy.signal import hilbert
import time
import psutil  # For memory tracking
from tqdm import tqdm

# Function to load images and labels from a directory
def load_data_from_directory(directory):
    X = []
    y = []
    original_images = []  # To store original images for visualization

    for label in os.listdir(directory):
        label_dir = os.path.join(directory, label)
        if os.path.isdir(label_dir):
            for filename in os.listdir(label_dir):
                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Add more image formats as needed
                    img_path = os.path.join(label_dir, filename)
                    image = cv2.imread(img_path)
                    if image is None:
                        print(f"Failed to load image: {img_path}")
                        continue

                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                    image_resized = cv2.resize(image, (128, 128))

                    # Extract HOG features
                    hog_features = hog(image_resized, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=False)

                    # Apply Local Mean Decomposition (LMD)
                    signal = np.mean(image_resized, axis=0)
                    analytic_signal = hilbert(signal)
                    amplitude_envelope = np.abs(analytic_signal)
                    phase = np.unwrap(np.angle(analytic_signal))
                    frequency = np.diff(phase) / (2.0 * np.pi)

                    combined_features = np.concatenate([hog_features, amplitude_envelope, frequency])

                    X.append(combined_features)

                    # Update labels for binary classification
                    if label == 'flawed':
                        y.append(1)
                    elif label == 'not flawed':
                        y.append(0)
                    else:
                        print(f"Unknown label: {label}. Skipping image {filename}.")
                        continue  # Skip if label is unrecognized

                    original_images.append(image_resized)

    # Ensure consistent lengths
    if len(X) != len(y):
        print(f"Mismatch in data: {len(X)} features vs {len(y)} labels.")
        min_len = min(len(X), len(y))
        X = X[:min_len]
        y = y[:min_len]
        original_images = original_images[:min_len]

    return np.array(X), np.array(y), np.array(original_images)

# Define PyTorch Dataset class
class CustomDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)

# Load training data from directories
train_dir = '/home/team39/Documents/ANN/train'
X_train, y_train, original_train_images = load_data_from_directory(train_dir)

# Confirm data consistency
print(f"Data Loaded - Features: {X_train.shape}, Labels: {y_train.shape}")

# Split dataset into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Create PyTorch Datasets and DataLoaders
train_dataset = CustomDataset(X_train_split, y_train_split)
val_dataset = CustomDataset(X_val_split, y_val_split)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Define the PyTorch Model
class ANNModel(nn.Module):
    def __init__(self):
        super(ANNModel, self).__init__()
        self.fc1 = nn.Linear(X_train_split.shape[1], 256)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 2)  # Binary classification: 2 output classes

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# Instantiate the model, loss function, and optimizer
model = ANNModel()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Track time for latency calculation
start_time = time.time()

# Lists to store the metrics
train_accuracies = []
val_accuracies = []
train_losses = []
val_losses = []

# Function to get memory usage in MB
def get_memory_usage():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return memory_info.rss / (1024 * 1024)  # Convert to MB

# Training loop
epochs = 100
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == labels).sum().item()
        total_samples += labels.size(0)

    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = correct_predictions / total_samples
    train_losses.append(epoch_loss)
    train_accuracies.append(epoch_accuracy)

    # Validation phase
    model.eval()
    val_loss = 0.0
    val_correct_predictions = 0
    val_total_samples = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct_predictions += (predicted == labels).sum().item()
            val_total_samples += labels.size(0)

    val_loss /= len(val_loader)
    val_accuracy = val_correct_predictions / val_total_samples
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    # Print progress every 10 epochs
    if epoch % 10 == 0:
        print(f"Epoch [{epoch}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

# Calculate total training time
end_time = time.time()
total_training_time = end_time - start_time

# Convert total time to minutes and seconds
minutes = total_training_time // 60
seconds = total_training_time % 60

print(f"Total training time: {int(minutes)} minutes and {seconds:.2f} seconds")

# Evaluate model performance
print(f"Model accuracy on validation data: {val_accuracy * 100:.2f}%")

# Plot training and validation accuracy
plt.figure(figsize=(12, 5))

# Plot Accuracy
plt.subplot(1, 2, 1)
plt.plot(range(epochs), train_accuracies, label='Training Accuracy')
plt.plot(range(epochs), val_accuracies, label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(range(epochs), train_losses, label='Training Loss')
plt.plot(range(epochs), val_losses, label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Show plots
plt.tight_layout()
plt.show()

# Save model with progress bar
def save_model_with_progress(model, filepath):
    temp_filepath = filepath + ".tmp"
    torch.save(model.state_dict(), temp_filepath)
    total_size = os.path.getsize(temp_filepath)

    with open(temp_filepath, "rb") as temp_file:
        with open(filepath, "wb") as final_file:
            with tqdm(total=total_size, unit="B", unit_scale=True, desc="Saving model") as pbar:
                while True:
                    chunk = temp_file.read(1024 * 1024)
                    if not chunk:
                        break
                    final_file.write(chunk)
                    pbar.update(len(chunk))

    os.remove(temp_filepath)
    print(f"Model successfully saved to {filepath}.")

save_model_with_progress(model, 'ANN_model.pth')

# Save the trained model
torch.save(model.state_dict(), 'ANN_model.pth')
